{"meta":{"title":"Welcome to Ray's blog","subtitle":"","description":"","author":"Ray Huang","url":"https://ruibinwong.com","root":"/"},"pages":[],"posts":[{"title":"MSc project work log - Self Organizing Feature Map 1","slug":"MSc-project-work-log-Self-Organizing-Feature-Map-1","date":"2020-06-18T11:47:17.000Z","updated":"2020-06-18T14:52:15.649Z","comments":true,"path":"2020/06/18/MSc-project-work-log-Self-Organizing-Feature-Map-1/","link":"","permalink":"https://ruibinwong.com/2020/06/18/MSc-project-work-log-Self-Organizing-Feature-Map-1/","excerpt":"","text":"Self Organizing Feature Map 1This experiment use SOFM (Self Organizing Feature Map) to clustering the models with Neupy package. SOFM algorithmThe SOFM algorithm is a kind of neural network designed for unsupervised learning, which could generate a neurons matrix (typically 2D matrix) to represent a group of samples. In this scenario, we use the hyperparameters as the code showing below. 12345678910from neupy import algorithmssofm = algorithms.SOFM( n_inputs=135, step=0.4, features_grid=(8, 8), learning_radius=8, std=1, verbose=True,) n_inputs: define the number of variables of the input samples step: define the training step size or learning rate features_grid: is the structure of the expected map. Here we define a (8, 8) matrix representing a rectangle map learning_radius: define how many neighbouring neurons would be updated when update a certain neuron is updating. Here is a diagram explain the learning radius affecting the map. The further description can be found in this [notebook](https://github.com/itdxer/neupy/blob/master/notebooks/sofm/Visualizations from article - “Self-Organizing Maps and Applications”.ipynb). ![](./MSc-project-work-log-Self-Organizing-Feature-Map-1/learning radius example.png) std: configure the learning rate of the neighbouring neurons. Learning rate scales based on the factors produced by the normal distribution with center in the place of a winning neuron and standard deviation specified as a parameter. verbose: controls the message printing out when the SOFM updating. Here is further explanation of the parameters in SOFM. We run this SOFM with 6000 epochs, took nearly 10 hours, and get the 64 (8*8) weights, or winner neurons more precisely, which could be regarded as the centres of groups. The next thing we should do is the visualization of what we have. VisualizationAt the beginning, we mapped the winner neurons to their true OIP since the winner neurons have 135 variables. We used the code below to match the closest model of these winner neurons and took the models true OIP as theirs. 12345678910indexs = []for w in weights: dis = 10**30 candidate_index = -1 for index, gen in enumerate(genomes): tmp_dis = distance(w, gen) if dis &gt; tmp_dis: dis = tmp_dis candidate_index = index indexs.append(candidate_index) There we have the heatmap of the OIP value in the feature map, the darker color of the cells meaning the higher OIP value. Also, we predicted the group they belong for all models, and dye their corresponding colour. The red cross on the map is the winner neurons. The colour is roughly coherent with their true OIP level. The majority of the high OIP models have darker colour while the lower OIP models are lighter. The winner neurons are discrete among the models. This is the average distance heatmap of all neurons. Basically, it is similar with the OIP heatmap. We also combine the OIP heatmap and distance heatmap together. The blue cells represent the neurons and the grey cells are the value of the distance between two neurons. The higher value they are, the darker colour they have.","categories":[],"tags":[{"name":"MSc project","slug":"MSc-project","permalink":"https://ruibinwong.com/tags/MSc-project/"}]},{"title":"MSc project work log - Gradient Boosting Regression 2","slug":"MSc-project-work-log-Gradient-Boosting-Regression-2","date":"2020-06-04T07:40:59.000Z","updated":"2020-06-08T16:11:09.876Z","comments":true,"path":"2020/06/04/MSc-project-work-log-Gradient-Boosting-Regression-2/","link":"","permalink":"https://ruibinwong.com/2020/06/04/MSc-project-work-log-Gradient-Boosting-Regression-2/","excerpt":"","text":"Gradient Boosting Regression 2We repeated the GB regression experiment 30 times since the GB regression has the unstable output, and collect the data for calculating the R square score to see the performance. The plots shows that the The training set ratio. The number correspond with the ratio below. (e.g. No.2 means the train set ratio is 0.01) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 0.005 0.01 0.016 0.021 0.026 0.031 0.036 0.042 0.047 0.052 0.057 0.062 0.068 0.073 0.078 0.083 0.089 0.094 0.099 0.104 0.109 0.115 0.12 0.125 From the box plots, we can see the GB regressor shows good performance on very small samples (72 samples will give the good result).","categories":[],"tags":[{"name":"MSc project","slug":"MSc-project","permalink":"https://ruibinwong.com/tags/MSc-project/"}]},{"title":"MSc project work log - Project Preparation report review","slug":"MSc-project-work-log-Project-Preparation-report-review","date":"2020-06-04T04:18:07.000Z","updated":"2020-06-04T08:00:10.287Z","comments":true,"path":"2020/06/04/MSc-project-work-log-Project-Preparation-report-review/","link":"","permalink":"https://ruibinwong.com/2020/06/04/MSc-project-work-log-Project-Preparation-report-review/","excerpt":"","text":"Project Preparation report reviewTechnical Understanding The report fails to explain the technical parts of the project to a reader unfamiliar with the topic. This leads to the conclusion that the student is confused and does not understand the technical parts of the project. Try to de-couple the project work (cluster geological models under certain criteria) from the underlying motivation (how the geological models are generated and represented). It seems like I am confused about the definition of the word “Motivation” and I don’t know what content should be in Motivation section. In Wikipedia, Motivation is the experience of desire or aversion (you want something, or want to avoid or escape something). In the PP report, I mostly wrote the context but ignore to mentioned my motivation which is something I want to do. Also, I think the motivation and objective is the same. The main motivation, background literature and general structure of the work is confused and unclear. The content is generally not structured under the expected headings (such as project objectives discussed in Literature sections.) I did not write the proper content in these sections. It is means that I did not understand the meanings of motivation. The objectives are not clear either – Are we reducing computational effort, manual analysis effort, data production effort or something else? It is mentioned in section 1.2 The student appears to have explored the data and done some initial work but it is unclear if this is of high quality or relevant. Formal ProcessThe workplan looks a little ambitious, and the student should consider keeping notes throughout the whole dissertation period especially if they plan to start writing the dissertation report in the last phase of the work. The risks are mostly generic. Production Quality The language of the report is often hard to follow, but the general attention to details is acceptable. The report has too many headings in some parts (e.g., in the introduction, in Section 2.3.x). I should polish my English writing. It would have been good to have seen more figures helping explain the overall aims of the project. Critical Review Previous Disse Review of previous dissertation is good. The end paragraph is 2.3.1 is confused which mean I don’t know if the student understood the point or has simply messed up explaining it. This is a shame as their paragraph was the main one in which the student is critically evaluating the dissertation. There is no comment on whether the project was successful, and whether the dissertation report reflects on its objectives. It seems like I did not write enough content that the readers expected. I repeated the original content but I did not comment on the success and the completion of the objective.","categories":[],"tags":[{"name":"MSc project","slug":"MSc-project","permalink":"https://ruibinwong.com/tags/MSc-project/"}]},{"title":"MSc project work log - Gradient Boosting Regression 1","slug":"MSc-project-research-journey-Gradient-Boosting-Regression-1","date":"2020-05-17T16:26:04.000Z","updated":"2020-05-26T10:01:28.635Z","comments":true,"path":"2020/05/18/MSc-project-research-journey-Gradient-Boosting-Regression-1/","link":"","permalink":"https://ruibinwong.com/2020/05/18/MSc-project-research-journey-Gradient-Boosting-Regression-1/","excerpt":"","text":"Gradient Boosting Regression 1We thought the sampling method should be periodically since the model sequence is also periodical. We design 2 sampling methods: 1. choose samples from the group belonging with the same gene (could be same Sw, NTG or Phi). 2. choose samples randomly. 1234567891011121314151617181920def sampling(mode='Kgroup', k=1): train_indexes = [] if mode == 'Kgroup': for i in range(24): _2st_indexes = [] for j in range(24): _2st_indexes += list(range(24 ** 3))[j * 24 ** 2 + i * 24: j * 24 ** 2 + (i + 1) * 24:] train_indexes += random.sample(list(range(24 ** 3))[i::24], k=k) # 3rd gene sampling train_indexes += random.sample(_2st_indexes, k=k) # 2nd gene sampling train_indexes += random.sample(list(range(24 ** 3))[i * (24 ** 2):(i + 1) * (24 ** 2)],k=k) # 1st gene sampling else: train_indexes = random.sample(list(range(24**3)), k=24*3*k) train_indexes = set(train_indexes) test_indexes = set(list(range(24 ** 3))) - train_indexes train_indexes = list(train_indexes) test_indexes = list(test_indexes) return train_indexes, test_indexes Training code: 1234567891011121314def train(mode, k): train_indexes, test_indexes = sampling(mode, k) genes = dataprovider.read_genomes(drop_stratigraphy_keys=True) OIP = dataprovider.read_OIP_true() x_train = genes[train_indexes] y_train = OIP[train_indexes] x_test = genes y_test = OIP reg = GradientBoostingRegressor(random_state=0) reg.fit(x_train, y_train) y_pred = reg.predict(x_test) return r2_score(y_test, y_pred) We experimented 24 times of Gradient boosting regression with different ratios of training set and gave the result on the figure above. From the figure, we could know the sampling method does not really affect the regression quality but the regression experiment obtain a good result.","categories":[],"tags":[{"name":"MSc project","slug":"MSc-project","permalink":"https://ruibinwong.com/tags/MSc-project/"}]},{"title":"MSc project work log - Gene investigation 1","slug":"MSc-project-research-journey-Gene-investigation-1","date":"2020-05-17T06:38:14.000Z","updated":"2020-05-26T10:01:14.080Z","comments":true,"path":"2020/05/17/MSc-project-research-journey-Gene-investigation-1/","link":"","permalink":"https://ruibinwong.com/2020/05/17/MSc-project-research-journey-Gene-investigation-1/","excerpt":"","text":"Gene Investigation 1 - Quality of FitWe found the trends of QoF (Quality of Fit) are related with OIP value.","categories":[],"tags":[{"name":"MSc project","slug":"MSc-project","permalink":"https://ruibinwong.com/tags/MSc-project/"}]},{"title":"MSc project work log - OIP investigation 3","slug":"MSc-project-research-journey-OIP-investigation-3","date":"2020-05-14T11:21:55.000Z","updated":"2020-05-26T10:01:40.821Z","comments":true,"path":"2020/05/14/MSc-project-research-journey-OIP-investigation-3/","link":"","permalink":"https://ruibinwong.com/2020/05/14/MSc-project-research-journey-OIP-investigation-3/","excerpt":"","text":"OIP investigation 3The conclusion of OIP investigation 2 can be more directly to observe when we reorder the model number. The model sequence in the data file is ordered like NTG 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 … Phi 0 0 0 … 0 1 1 1 … 1 … 24 0 0 0 … 0 1 1 1 … 1 … 24 … Sw 0 1 2 … 24 0 1 2 … 24 … 24 0 1 2 … 24 0 1 2 … 24 … 24 … and then we have the figure like Once we reorder the model sequence like Sw 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 … Phi 0 0 0 … 0 1 1 1 … 1 … 24 0 0 0 … 0 1 1 1 … 1 … 24 … NTG 0 1 2 … 24 0 1 2 … 24 … 24 0 1 2 … 24 0 1 2 … 24 … 24 … we will get the figure like This figure would be more clear since the models’ OIP is grouping together and showing the difference of OIP level. The OIP curves with Phi genes changing within each Sw gene is showed as the figure below. The different colour curves are the OIP value within different NTG genes.","categories":[],"tags":[{"name":"MSc project","slug":"MSc-project","permalink":"https://ruibinwong.com/tags/MSc-project/"}]},{"title":"MSc project work log - OIP investigation 2","slug":"MSc-project-research-journey-OIP-investigation-2","date":"2020-05-12T16:20:14.000Z","updated":"2020-05-26T10:01:36.665Z","comments":true,"path":"2020/05/13/MSc-project-research-journey-OIP-investigation-2/","link":"","permalink":"https://ruibinwong.com/2020/05/13/MSc-project-research-journey-OIP-investigation-2/","excerpt":"","text":"OIP investigation 2In OIP investigation 1, we split the OIP curve to fragments according to the Phi gene. This blog records the insight for discussing the NTG and water saturation affecting the fragments. Net to Gross (NTG)We draw the different colour OIP curve to discriminate NTG genes and conclude that the NTG genes influence the range of OIP values. The figure displays the 24 different colours dots separating vertically as bars, representing the 24 NTG genes. Each bar has different maximum and minimum OIP values but a similar distribution. We, therefore, assume that the NTG gene affects the range of the group of OIP values. Water saturation (Sw)The water saturation mostly affect the OIP value as the figure below showing. The OIP values from the models with the same Sw gene (the same colour) show the approximate consistency. OverallIn conclusion, we have known the way of model’s genes affecting the OIP values. However, we should not see these insights in the task since, in the scenario, we do not know the OIP value, and we only have the genes. So we should focus on analysing the variables from the genes to see if there is consistency with OIP value. Hopefully, we can get a proper unsupervised grouping metrics from it.","categories":[],"tags":[{"name":"MSc project","slug":"MSc-project","permalink":"https://ruibinwong.com/tags/MSc-project/"}]},{"title":"MSc project work log - OIP investigation 1","slug":"MSc-project-research-journey-OIP-investigation-1","date":"2020-05-08T11:41:16.000Z","updated":"2020-05-26T10:01:05.016Z","comments":true,"path":"2020/05/08/MSc-project-research-journey-OIP-investigation-1/","link":"","permalink":"https://ruibinwong.com/2020/05/08/MSc-project-research-journey-OIP-investigation-1/","excerpt":"","text":"OIP investigation 1We tried to explore the OIP value curve from Phi, NTG and Water Saturation. We found a proper division method that treating the OIP plotted by 24 Phi genes as a fragment, and these fragments could show a similar level of OIP (as the figure showing below). NTG and Water Saturation decide the positions of fragments on the figure. These positions reflect the different level of OIP, which is what we are expecting. We should do further exploration of NTG and Water Saturation to see how they influence the fragments’ positions. Now we can be sure these genes are really affecting the OIP.","categories":[],"tags":[{"name":"MSc project","slug":"MSc-project","permalink":"https://ruibinwong.com/tags/MSc-project/"}]}],"categories":[],"tags":[{"name":"MSc project","slug":"MSc-project","permalink":"https://ruibinwong.com/tags/MSc-project/"}]}